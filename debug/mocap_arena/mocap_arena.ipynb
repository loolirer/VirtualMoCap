{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoCap Arena\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules...\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cv2\n",
    "import time\n",
    "import socket\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..') # Go back to base directory\n",
    "\n",
    "from modules.vision.camera import Camera\n",
    "from modules.vision.epipolar_geometry import build_essential_matrix, build_fundamental_matrix, order_centroids\n",
    "from modules.vision.blob_detection import detect_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciating Camera Objects\n",
    "\n",
    "To generate the data regarding the mathematical model of the vision sensor in *CoppeliaSim*, it will be instanciated a Camera Obejcts that matches the Vision Sensors' intrinsic and extrinsic parameters. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cameras = 4\n",
    "camera = [] # Camera objects list\n",
    "\n",
    "# Object matrix of Camera 0\n",
    "base_matrix = np.array([[-7.07106781e-01,  5.00000000e-01, -5.00000000e-01, 2.50000000e+00],\n",
    "                        [ 7.07106781e-01,  5.00000000e-01, -5.00000000e-01, 2.50000000e+00],\n",
    "                        [ 1.46327395e-13, -7.07106781e-01, -7.07106781e-01, 2.50000000e+00]])\n",
    "\n",
    "for ID in range(n_cameras):\n",
    "    # Spread all cameras uniformely in a circle around the arena\n",
    "    R = np.array(sp.spatial.transform.Rotation.from_euler('z', (360 / n_cameras) * ID, degrees=True).as_matrix())\n",
    "    object_matrix = R @ base_matrix\n",
    "\n",
    "    camera.append(Camera(# Intrinsic Parameters\n",
    "                         resolution=(720,720), \n",
    "                         fov_degrees=60.0,     \n",
    "         \n",
    "                         # Extrinsic Parameters\n",
    "                         object_matrix=object_matrix,\n",
    "         \n",
    "                         # Rational Lens Distortion Model\n",
    "                         # distortion_model='rational',\n",
    "                         # distortion_coefficients=np.array([0.014, -0.003, -0.0002, -0.000003, 0.0009, 0.05, -0.007, 0.0017]), \n",
    "                         \n",
    "                         # Fisheye Lens Distortion Model\n",
    "                         distortion_model='fisheye',\n",
    "                         distortion_coefficients=np.array([0.395, 0.633, -2.417, 2.110]),\n",
    "         \n",
    "                         # Image Noise Model\n",
    "                         snr_dB=13\n",
    "                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Fundamental Matrices\n",
    "\n",
    "For each pair $\\{i, j\\}$, a fundamental matrix will be calculated between camera's $i$ and $j$. In this case, this information will be storted in a matrix $F$ such that $F_{ij}$ will be the fundamental matrix between camera's $i$ and $j$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_matrix = np.array(np.zeros((n_cameras, n_cameras, 3, 3)))\n",
    "\n",
    "for reference in range(n_cameras):\n",
    "    for auxiliary in range(n_cameras):\n",
    "        if reference == auxiliary:\n",
    "            continue\n",
    "\n",
    "        E = build_essential_matrix(camera[reference].extrinsic_matrix, camera[auxiliary].extrinsic_matrix)\n",
    "\n",
    "        F = build_fundamental_matrix(camera[reference].intrinsic_matrix, camera[auxiliary].intrinsic_matrix, E)\n",
    "\n",
    "        fundamental_matrix[reference][auxiliary] = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Server's UDP Socket\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SERVER] Creating socket...\n",
      "[SERVER] Socket successfully created\n",
      "[SERVER] Bound to port 8888\n"
     ]
    }
   ],
   "source": [
    "tag = 'SERVER' \n",
    "\n",
    "print(f'[{tag}] Creating socket...')\n",
    "\n",
    "buffer_size = 1024   # In bytes\n",
    "\n",
    "# Try to create server socket\n",
    "try: \n",
    "    server_socket = socket.socket(socket.AF_INET,    # Internet\n",
    "                                  socket.SOCK_DGRAM) # UDP\n",
    "    print(f'[{tag}] Socket successfully created')\n",
    "    \n",
    "except socket.error as err: \n",
    "    print(f'[{tag}] Socket creation failed with error {err}\\n')\n",
    "    print(f'[{tag}] Quitting code...')\n",
    "    exit()\n",
    "\n",
    "server_ip = '127.0.0.1' # Server IP\n",
    "server_port = 8888      # Server Port\n",
    "server_address = (server_ip, server_port) \n",
    "\n",
    "server_socket.bind(server_address)\n",
    "print(f'[{tag}] Bound to port {server_port}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Arena Controller\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SERVER] Capture info sent\n",
      "[SERVER] Scene ready\n",
      "[SERVER] Trigger sent!\n"
     ]
    }
   ],
   "source": [
    "n_clients = 4\n",
    "simulation_time = 10 # In seconds\n",
    "\n",
    "# Controller socket info\n",
    "controller_ip = '127.0.0.1' # Controller IP\n",
    "controller_port = 7777      # Controller Port\n",
    "controller_address = (controller_ip, controller_port)\n",
    "\n",
    "# Send capture info to the controller\n",
    "message = f'{n_clients},{simulation_time}'\n",
    "\n",
    "message_bytes = message.encode()\n",
    "\n",
    "server_socket.sendto(message_bytes, controller_address)\n",
    "print(f'[{tag}] Capture info sent')\n",
    "\n",
    "# Wait for controller to setup the scene\n",
    "server_socket.recvfrom(buffer_size)\n",
    "print(f'[{tag}] Scene ready')\n",
    "\n",
    "# Send trigger\n",
    "server_socket.sendto(''.encode(), controller_address)\n",
    "print(f'[{tag}] Trigger sent!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Clients Addresses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SERVER] Waiting for clients...\n",
      "\tClient 0 connected\n",
      "\tClient 1 connected\n",
      "\tClient 2 connected\n",
      "\tClient 3 connected\n",
      "[SERVER] All clients connected!\n"
     ]
    }
   ],
   "source": [
    "client_addresses = {}\n",
    "\n",
    "print(f'[{tag}] Waiting for clients...')\n",
    "\n",
    "# Address lookup \n",
    "while len(client_addresses.keys()) < n_clients: # Until all clients are identified\n",
    "    message_bytes, address = server_socket.recvfrom(buffer_size)\n",
    "\n",
    "    try:\n",
    "        ID = int(message_bytes.decode()) # Decode message\n",
    "\n",
    "    except: # Invalid message for decoding\n",
    "        continue # Look for another message\n",
    "\n",
    "    client_addresses[address] = ID\n",
    "\n",
    "    print(f'\\tClient {ID} connected')\n",
    "\n",
    "print(f'[{tag}] All clients connected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Simulation\n",
    "\n",
    "For a running simulation, in each Presentation Timestamp (PTS), the following process will happen:\n",
    "\n",
    "1. Get images of all cameras;\n",
    "2. Detect blobs;\n",
    "3. Undistort detected blob coordinates;\n",
    "4. Order blobs based on epipolar lines;\n",
    "5. Triangulate markers.\n",
    "\n",
    "For each PTS the data will be saved for later analysis.\n",
    "\n",
    "To evaluate how well the system is able to process at run-time, the Real Time Factor of the system will be calculated:\n",
    "\n",
    "$${RTF} = \\frac{t_S}{t_R}$$\n",
    "\n",
    "Where $t_S$ is the total simulation time, and $t_R$ is the total ammount of real time that the system took to ran.\n",
    "\n",
    "The `synchronous` flag will define if the *CoppeliaSim* simulation will wait for the trigger to go to its next step. If so, the system is synchronous, if not, the simulation will run by itself and will not wait for the calculations to be done in the code, making the system asynchronous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Synchronous system flag\\nsynchronous = True\\n\\n# Make system synchronous\\nsim.setStepping(synchronous)\\n\\nsimulation_time = 10 # In seconds\\nsimulation_step = sim.getSimulationTimeStep()\\n\\n# Triangulation pair\\npair = (0, 1)\\nreference, auxiliary = pair\\n\\nimages = {} # Images of all cameras in each presentation timestamps\\ntrue_marker_positions = {} # Ground truth marker positions in each presentation timestamps\\nestimated_marker_positions = {} # Triangulated marker positions in each presentation timestamps\\n\\n# Simulation begins here\\nsim.startSimulation()\\n\\n# Create triangulation tag\\ntag_handle = sim.createDummy(0.015)\\n\\n# Change triangulation tag color properties\\nsim.setObjectColor(tag_handle, 0, sim.colorcomponent_ambient_diffuse, list([0, 0, 0]))\\nsim.setObjectColor(tag_handle, 0, sim.colorcomponent_specular,        list([0, 0, 0])) \\nsim.setObjectColor(tag_handle, 0, sim.colorcomponent_emission,        list([0, 1, 0])) \\n\\ntimer_start = time.time() # Start timer\\n\\nwhile (t := sim.getSimulationTime()) < simulation_time:\\n    # Get and save images\\n    sync_images = [camera[ID].get_coppelia_image(sim.getVisionSensorImg) for ID in pair]\\n    images[t] = sync_images \\n\\n    # Detect blobs on the distorted image\\n    distorted_blobs = [detect_blobs(sync_images[ID]) for ID in pair]\\n\\n    # If no blobs were detected then images are invalid\\n    if any(blob is None for blob in distorted_blobs):\\n        if synchronous:\\n            sim.step()\\n        continue # Jump to next iteration\\n    \\n    # If blob count is different between each image\\n    if distorted_blobs[reference].shape != distorted_blobs[auxiliary].shape:\\n        if synchronous:\\n            sim.step()\\n        continue # Jump to next iteration\\n    \\n    # Undistort blobs\\n    undistorted_blobs = [camera[ID].undistort_points(distorted_blobs[ID].T.reshape(1, -1, 2).astype(np.float32), \\n                                                     camera[ID].intrinsic_matrix, \\n                                                     camera[ID].distortion_coefficients,\\n                                                     np.array([]),\\n                                                     camera[ID].intrinsic_matrix).reshape(-1,2).T for ID in pair]\\n\\n    # Order blobs\\n    epilines_auxiliary = cv2.computeCorrespondEpilines(points=undistorted_blobs[reference].T, \\n                                                       whichImage=1, \\n                                                       F=fundamental_matrix[reference][auxiliary]).reshape(-1,3)\\n    \\n    undistorted_blobs[auxiliary] = order_centroids(undistorted_blobs[auxiliary], epilines_auxiliary)\\n\\n    # Triangulate markers\\n    triangulated_positions = cv2.triangulatePoints(camera[reference].projection_matrix.astype(float), \\n                                                   camera[auxiliary].projection_matrix.astype(float), \\n                                                   undistorted_blobs[reference].astype(float), \\n                                                   undistorted_blobs[auxiliary].astype(float))\\n    \\n    # Normalize homogeneous coordinates and save\\n    triangulated_positions = (triangulated_positions / triangulated_positions[-1])[:-1, :]\\n    estimated_marker_positions[t] = triangulated_positions\\n\\n    # Position triangulation tag\\n    sim.setObjectPosition(tag_handle, triangulated_positions.reshape(-1, 3)[0].tolist())\\n\\n    # Get ground truth marker position for comparison\\n    true_marker_position = np.array(sim.getObjectPosition(sim.getObject('/Marker'))).reshape(3,-1)\\n    true_marker_positions[t] = true_marker_position\\n\\n    # Call next simulation step\\n    if synchronous:\\n        sim.step()\\n\\ntimer_end = time.time() # Stop timer\\n\\n# Simulation ends here\\nsim.stopSimulation()\\n\\nprint(f'Real Time Factor: {simulation_time / (timer_end - timer_start)}')\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Synchronous system flag\n",
    "synchronous = True\n",
    "\n",
    "# Make system synchronous\n",
    "sim.setStepping(synchronous)\n",
    "\n",
    "simulation_time = 10 # In seconds\n",
    "simulation_step = sim.getSimulationTimeStep()\n",
    "\n",
    "# Triangulation pair\n",
    "pair = (0, 1)\n",
    "reference, auxiliary = pair\n",
    "\n",
    "images = {} # Images of all cameras in each presentation timestamps\n",
    "true_marker_positions = {} # Ground truth marker positions in each presentation timestamps\n",
    "estimated_marker_positions = {} # Triangulated marker positions in each presentation timestamps\n",
    "\n",
    "# Simulation begins here\n",
    "sim.startSimulation()\n",
    "\n",
    "# Create triangulation tag\n",
    "tag_handle = sim.createDummy(0.015)\n",
    "\n",
    "# Change triangulation tag color properties\n",
    "sim.setObjectColor(tag_handle, 0, sim.colorcomponent_ambient_diffuse, list([0, 0, 0]))\n",
    "sim.setObjectColor(tag_handle, 0, sim.colorcomponent_specular,        list([0, 0, 0])) \n",
    "sim.setObjectColor(tag_handle, 0, sim.colorcomponent_emission,        list([0, 1, 0])) \n",
    "\n",
    "timer_start = time.time() # Start timer\n",
    "\n",
    "while (t := sim.getSimulationTime()) < simulation_time:\n",
    "    # Get and save images\n",
    "    sync_images = [camera[ID].get_coppelia_image(sim.getVisionSensorImg) for ID in pair]\n",
    "    images[t] = sync_images \n",
    "\n",
    "    # Detect blobs on the distorted image\n",
    "    distorted_blobs = [detect_blobs(sync_images[ID]) for ID in pair]\n",
    "\n",
    "    # If no blobs were detected then images are invalid\n",
    "    if any(blob is None for blob in distorted_blobs):\n",
    "        if synchronous:\n",
    "            sim.step()\n",
    "        continue # Jump to next iteration\n",
    "    \n",
    "    # If blob count is different between each image\n",
    "    if distorted_blobs[reference].shape != distorted_blobs[auxiliary].shape:\n",
    "        if synchronous:\n",
    "            sim.step()\n",
    "        continue # Jump to next iteration\n",
    "    \n",
    "    # Undistort blobs\n",
    "    undistorted_blobs = [camera[ID].undistort_points(distorted_blobs[ID].T.reshape(1, -1, 2).astype(np.float32), \n",
    "                                                     camera[ID].intrinsic_matrix, \n",
    "                                                     camera[ID].distortion_coefficients,\n",
    "                                                     np.array([]),\n",
    "                                                     camera[ID].intrinsic_matrix).reshape(-1,2).T for ID in pair]\n",
    "\n",
    "    # Order blobs\n",
    "    epilines_auxiliary = cv2.computeCorrespondEpilines(points=undistorted_blobs[reference].T, \n",
    "                                                       whichImage=1, \n",
    "                                                       F=fundamental_matrix[reference][auxiliary]).reshape(-1,3)\n",
    "    \n",
    "    undistorted_blobs[auxiliary] = order_centroids(undistorted_blobs[auxiliary], epilines_auxiliary)\n",
    "\n",
    "    # Triangulate markers\n",
    "    triangulated_positions = cv2.triangulatePoints(camera[reference].projection_matrix.astype(float), \n",
    "                                                   camera[auxiliary].projection_matrix.astype(float), \n",
    "                                                   undistorted_blobs[reference].astype(float), \n",
    "                                                   undistorted_blobs[auxiliary].astype(float))\n",
    "    \n",
    "    # Normalize homogeneous coordinates and save\n",
    "    triangulated_positions = (triangulated_positions / triangulated_positions[-1])[:-1, :]\n",
    "    estimated_marker_positions[t] = triangulated_positions\n",
    "\n",
    "    # Position triangulation tag\n",
    "    sim.setObjectPosition(tag_handle, triangulated_positions.reshape(-1, 3)[0].tolist())\n",
    "\n",
    "    # Get ground truth marker position for comparison\n",
    "    true_marker_position = np.array(sim.getObjectPosition(sim.getObject('/Marker'))).reshape(3,-1)\n",
    "    true_marker_positions[t] = true_marker_position\n",
    "\n",
    "    # Call next simulation step\n",
    "    if synchronous:\n",
    "        sim.step()\n",
    "\n",
    "timer_end = time.time() # Stop timer\n",
    "\n",
    "# Simulation ends here\n",
    "sim.stopSimulation()\n",
    "\n",
    "print(f'Real Time Factor: {simulation_time / (timer_end - timer_start)}')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Precision and Accuracy \n",
    "\n",
    "Since the ground truth data of the marker's position is available, it's possible to evaluate the Precision and Accuracy of the system for the capture of a single stationary marker on the scene.\n",
    "\n",
    "Calculating the average difference between the position of the marker it's ground truth will determine the **Precision**. Calculating the standard deviation of the marker's position will determine how accurate the system is by it's **Jitter**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"static_capture = False # Flag if the marker is moving or not in the scene \\n\\n# Gathering Data\\nall_marker_positions = None\\ntriangulation_errors = None\\nfor frame, pts in enumerate(estimated_marker_positions.keys()):\\n    if all_marker_positions is None:\\n        all_marker_positions = estimated_marker_positions[pts]\\n    else:\\n        all_marker_positions = np.hstack((all_marker_positions, estimated_marker_positions[pts]))\\n\\n    if triangulation_errors is None:\\n        triangulation_errors = np.linalg.norm(estimated_marker_positions[pts] - true_marker_positions[pts])\\n    else:\\n        triangulation_errors = np.hstack((triangulation_errors, np.linalg.norm(estimated_marker_positions[pts] - true_marker_positions[pts])))\\n\\n# For static single marker capture\\nif static_capture == True:\\n    true_position = np.ravel(list(true_marker_positions.values())[0]) # Ground truth\\n    average_position = np.mean(all_marker_positions, axis=1) # For Precision\\n    std_dev_position = np.std(all_marker_positions, axis=1)  # For Accuracy \\n\\n    print('Ground Truth Position:', true_position)\\n    print('Average Position:', average_position) \\n    print(f'Average Reconstruction Error: {np.mean(triangulation_errors) * 1e3 :.2f} mm') # Precision\\n    print(f'Standard Deviation: {np.linalg.norm(std_dev_position) * 1e3 :.2f} mm')        # Accuracy\\n\\n# For moving single marker capture\\nelse:\\n    print(f'Average Reconstruction Error: {np.mean(triangulation_errors) * 1e3 :.2f} mm') # Precision\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''static_capture = False # Flag if the marker is moving or not in the scene \n",
    "\n",
    "# Gathering Data\n",
    "all_marker_positions = None\n",
    "triangulation_errors = None\n",
    "for frame, pts in enumerate(estimated_marker_positions.keys()):\n",
    "    if all_marker_positions is None:\n",
    "        all_marker_positions = estimated_marker_positions[pts]\n",
    "    else:\n",
    "        all_marker_positions = np.hstack((all_marker_positions, estimated_marker_positions[pts]))\n",
    "\n",
    "    if triangulation_errors is None:\n",
    "        triangulation_errors = np.linalg.norm(estimated_marker_positions[pts] - true_marker_positions[pts])\n",
    "    else:\n",
    "        triangulation_errors = np.hstack((triangulation_errors, np.linalg.norm(estimated_marker_positions[pts] - true_marker_positions[pts])))\n",
    "\n",
    "# For static single marker capture\n",
    "if static_capture == True:\n",
    "    true_position = np.ravel(list(true_marker_positions.values())[0]) # Ground truth\n",
    "    average_position = np.mean(all_marker_positions, axis=1) # For Precision\n",
    "    std_dev_position = np.std(all_marker_positions, axis=1)  # For Accuracy \n",
    "\n",
    "    print('Ground Truth Position:', true_position)\n",
    "    print('Average Position:', average_position) \n",
    "    print(f'Average Reconstruction Error: {np.mean(triangulation_errors) * 1e3 :.2f} mm') # Precision\n",
    "    print(f'Standard Deviation: {np.linalg.norm(std_dev_position) * 1e3 :.2f} mm')        # Accuracy\n",
    "\n",
    "# For moving single marker capture\n",
    "else:\n",
    "    print(f'Average Reconstruction Error: {np.mean(triangulation_errors) * 1e3 :.2f} mm') # Precision'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Back the Camera's Feed\n",
    "\n",
    "The following cell will replicate a real time camera feed of the simulation. Change the `ID` parameter to switch between camera views.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Playback a camera image feed in real time\\nID = 0 # Camera ID to be watched\\ntimestep = int(1e3 * simulation_time/len(images.keys())) # In milliseconds\\nfor frame, pts in enumerate(images.keys()):\\n    cv2.imshow(f'Camera {ID}', images[pts][ID])\\n    cv2.waitKey(timestep)\\n\\n# Closing all open windows \\ncv2.destroyAllWindows()\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Playback a camera image feed in real time\n",
    "ID = 0 # Camera ID to be watched\n",
    "timestep = int(1e3 * simulation_time/len(images.keys())) # In milliseconds\n",
    "for frame, pts in enumerate(images.keys()):\n",
    "    cv2.imshow(f'Camera {ID}', images[pts][ID])\n",
    "    cv2.waitKey(timestep)\n",
    "\n",
    "# Closing all open windows \n",
    "cv2.destroyAllWindows()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
