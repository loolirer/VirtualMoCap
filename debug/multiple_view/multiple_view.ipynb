{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronous Multiple View\n",
    "\n",
    "This notebook is an implementation, debugging and analysis of a Multiple View Synchronous Motion Capture System. \n",
    "\n",
    "The goal is to implement and analyse, how all of the parameters and models used will affect the overall triangulation. For this:\n",
    "1. Instanciate a set of Vision Sensors in both *CoppeliaSim*;\n",
    "2. Build Fundamental Matrices for each pair of instanciated cameras;\n",
    "3. Choose a pair for triangulation;\n",
    "4. In simulation, get images for each PTS and triangulate the points in each one;\n",
    "5. Check Real Time Factor, Triangulation Error and Jitter. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules...\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..') # Go back to base directory\n",
    "\n",
    "from modules.vision.camera import Camera\n",
    "from modules.vision.epipolar_geometry import build_essential_matrix, build_fundamental_matrix, order_centroids\n",
    "from modules.vision.blob_detection import detect_blobs\n",
    "\n",
    "from coppeliasim_zmqremoteapi_client import RemoteAPIClient\n",
    "\n",
    "# Init client \n",
    "client = RemoteAPIClient()    # Client object \n",
    "sim = client.getObject('sim') # Simulation object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciating Camera Objects\n",
    "\n",
    "To generate the data regarding the mathematical model of the vision sensor in *CoppeliaSim*, it will be instanciated a Camera Obejcts that matches the Vision Sensors' intrinsic and extrinsic parameters. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cameras = 8\n",
    "camera = [] # Camera objects list\n",
    "\n",
    "for ID in range(n_cameras):\n",
    "    # Get the vision sensor handle\n",
    "    vision_sensor_handle = sim.getObject(f'/VisionSensor[{ID}]')\n",
    "\n",
    "    # Get Coppelia's 3x4 object matrix\n",
    "    object_matrix = np.array(sim.getObjectMatrix(vision_sensor_handle)).reshape((3,4))\n",
    "\n",
    "    # X and Y axis of Coppelia's Vision Sensor are inverted\n",
    "    object_matrix[:,0] *= -1 # Invert x vector column\n",
    "    object_matrix[:,1] *= -1 # Invert y vector column\n",
    "\n",
    "    camera.append(Camera(# Simulation handling\n",
    "                          vision_sensor_handle=vision_sensor_handle,\n",
    "   \n",
    "                          # Intrinsic Parameters\n",
    "                          resolution=(720,720), \n",
    "                          fov_degrees=60.0,     \n",
    "          \n",
    "                          # Extrinsic Parameters\n",
    "                          object_matrix=object_matrix,\n",
    "          \n",
    "                          # Rational Lens Distortion Model\n",
    "                          # distortion_model='rational',\n",
    "                          # distortion_coefficients=np.array([0.014, -0.003, -0.0002, -0.000003, 0.0009, 0.05, -0.007, 0.0017]),\n",
    "\n",
    "                          # Fisheye Lens Distortion Model\n",
    "                          distortion_model='fisheye',\n",
    "                          distortion_coefficients=np.array([0.395, 0.633, -2.417, 2.110]),\n",
    "          \n",
    "                          # Image Noise Model\n",
    "                          snr_dB=13\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Fundamental Matrices\n",
    "\n",
    "For each pair $i, j$, a fundamental matrix will be calculated between camera's $i$ and $j$. In this case, this information will be storted in a matrix $F$ such that $F_{ij}$ will be the fundamental matrix between camera's $i$ and $j$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_matrix = np.array(np.zeros((n_cameras, n_cameras, 3, 3)))\n",
    "\n",
    "for reference in range(n_cameras):\n",
    "    for auxiliary in range(n_cameras):\n",
    "        if reference == auxiliary:\n",
    "            continue\n",
    "\n",
    "        E = build_essential_matrix(camera[reference].extrinsic_matrix, camera[auxiliary].extrinsic_matrix)\n",
    "\n",
    "        F = build_fundamental_matrix(camera[reference].intrinsic_matrix, camera[auxiliary].intrinsic_matrix, E)\n",
    "\n",
    "        fundamental_matrix[reference][auxiliary] = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Simulation\n",
    "\n",
    "For a running simulation, in each Presentation Timestamp (PTS), the following process will happen:\n",
    "\n",
    "1. Get images of all cameras;\n",
    "2. Detect blobs;\n",
    "3. Undistort detected blob coordinates;\n",
    "4. Order blobs based on epipolar lines;\n",
    "5. Triangulate markers.\n",
    "\n",
    "For each PTS the data will be saved for later analysis.\n",
    "\n",
    "To evaluate how well the system is able to process at run-time, the Real Time Factor of the system will be calculated:\n",
    "\n",
    "$${RTF} = \\frac{t_S}{t_R}$$\n",
    "\n",
    "Where $t_S$ is the total simulation time, and $t_R$ is the total ammount of real time that the code took to ran.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Time Factor: 0.4302817176659904\n"
     ]
    }
   ],
   "source": [
    "# When simulation is not running, ZMQ message handling could be a bit\n",
    "# slow, since the idle loop runs at 8 Hz by default.\n",
    "# Setting the idle loop to run at full speed for this program\n",
    "defaultIdleFps = sim.getInt32Param(sim.intparam_idle_fps)   \n",
    "sim.setInt32Param(sim.intparam_idle_fps, 0)\n",
    "\n",
    "pair = (0, 1)\n",
    "reference, auxiliary = pair\n",
    "\n",
    "\n",
    "simulation_time = 5 # In seconds\n",
    "images = {} # Images of all cameras in each presentation timestamps\n",
    "marker_positions = {} # Triangulated marker positions in each presentation timestamps\n",
    "\n",
    "# Simulation begins here\n",
    "sim.startSimulation()\n",
    "\n",
    "timer_start = time.time()\n",
    "\n",
    "while (t := sim.getSimulationTime()) < simulation_time:\n",
    "    # Get and save images\n",
    "    sync_images = [camera[ID].get_coppelia_image(sim.getVisionSensorCharImage) for ID in range(n_cameras)]\n",
    "    images[t] = sync_images    \n",
    "\n",
    "    # Detect blobs on the distorted image\n",
    "    distorted_blobs = [detect_blobs(sync_images[ID]) for ID in pair]\n",
    "\n",
    "    # If no blobs were detected then images are invalid\n",
    "    if any(blob is None for blob in distorted_blobs):\n",
    "        continue # Jump to next iteration\n",
    "        \n",
    "    # Undistort blobs\n",
    "    undistorted_blobs = [camera[ID].undistort_points(distorted_blobs[ID].T.reshape(1, -1, 2).astype(np.float32), \n",
    "                                                     camera[ID].intrinsic_matrix, \n",
    "                                                     camera[ID].distortion_coefficients,\n",
    "                                                     np.array([]),\n",
    "                                                     camera[ID].intrinsic_matrix).reshape(-1,2).T for ID in pair]\n",
    "\n",
    "    # Order blobs\n",
    "    epilines_auxiliary = cv2.computeCorrespondEpilines(points=undistorted_blobs[reference].T, \n",
    "                                                       whichImage=1, \n",
    "                                                       F=fundamental_matrix[reference][auxiliary]).reshape(-1,3)\n",
    "    \n",
    "    undistorted_blobs[auxiliary] = order_centroids(undistorted_blobs[auxiliary], epilines_auxiliary)\n",
    "\n",
    "    # Triangulate markers\n",
    "    triangulated_positions = cv2.triangulatePoints(camera[reference].projection_matrix.astype(float), \n",
    "                                                   camera[auxiliary].projection_matrix.astype(float), \n",
    "                                                   undistorted_blobs[reference].astype(float), \n",
    "                                                   undistorted_blobs[auxiliary].astype(float))\n",
    "    \n",
    "    # Normalize homogeneous coordinates and save\n",
    "    marker_positions[t] = (triangulated_positions / triangulated_positions[-1])[:-1, :]\n",
    "\n",
    "timer_end = time.time()\n",
    "\n",
    "# Closing all open windows \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Restore the original idle loop frequency:\n",
    "sim.setInt32Param(sim.intparam_idle_fps, defaultIdleFps)\n",
    "\n",
    "# Simulation ends here\n",
    "sim.stopSimulation()\n",
    "\n",
    "print(f'Real Time Factor: {simulation_time / (timer_end - timer_start)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Precision and Accuracy \n",
    "\n",
    "Since the ground truth data of the marker's position is available, it's possible to evaluate the Precision and Accuracy of the system for the capture of a single stationary marker on the scene.\n",
    "\n",
    "Calculating the average position of the marker and evaluating the difference between it's ground truth will determine the **Precision**. Calculating the standard deviation of the marker's position will determine how accurate the system is by it's **Jitter**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Position: [0.  0.  0.1]\n",
      "Average Position: [ 0.00360371 -0.00148805  0.09964503]\n",
      "Triangulation Error: 3.91 mm\n",
      "Standard Deviation: 1.76 mm\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy and Precision \n",
    "all_marker_positions = None\n",
    "for frame, pts in enumerate(marker_positions.keys()):\n",
    "    if all_marker_positions is None:\n",
    "        all_marker_positions = marker_positions[pts]\n",
    "    else:\n",
    "        all_marker_positions = np.hstack((all_marker_positions, marker_positions[pts]))\n",
    "\n",
    "true_position = np.array(sim.getObjectPosition(sim.getObject('/Marker')))\n",
    "average_position = np.mean(all_marker_positions, axis=1)\n",
    "std_dev_position = np.std(all_marker_positions, axis=1)\n",
    "\n",
    "print('Ground Truth Position:', true_position)\n",
    "print('Average Position:', average_position)\n",
    "print(f'Triangulation Error: {np.linalg.norm(average_position - true_position) * 1e3 :.2f} mm')\n",
    "print(f'Standard Deviation: {np.linalg.norm(std_dev_position) * 1e3 :.2f} mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Back the Camera's Feed\n",
    "\n",
    "The following cell will replicate a real time camera feed of the simulation. Change the `ID` parameter to switch between camera views.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playback a camera image feed in real time\n",
    "ID = 0 # Camera ID to be watched\n",
    "timestep = int(1e3 * simulation_time/len(images.keys()))\n",
    "for frame, pts in enumerate(images.keys()):\n",
    "    cv2.imshow(f'Camera {ID}', images[pts][ID])\n",
    "    cv2.waitKey(timestep)\n",
    "\n",
    "# Closing all open windows \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
