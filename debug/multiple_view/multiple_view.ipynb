{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Multiple View\n",
    "\n",
    "This notebook is an implementation, debugging and analysis of a an Multiple View Motion Capture System using *CoppeliaSim* as a renderer. \n",
    "\n",
    "The goal is to implement and analyse, how all of the parameters and models used will affect the overall triangulation. For this:\n",
    "1. Instanciate a set of Vision Sensors in *CoppeliaSim*;\n",
    "2. Build Fundamental Matrices for each pair of instanciated cameras;\n",
    "3. Choose a pair for triangulation;\n",
    "4. In simulation, get images for each PTS and triangulate the points in each one;\n",
    "5. Check Real Time Factor, Triangulation Error and Jitter. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules...\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..') # Go back to base directory\n",
    "\n",
    "from modules.vision.camera import Camera\n",
    "from modules.vision.epipolar_geometry import build_essential_matrix, build_fundamental_matrix, order_centroids\n",
    "from modules.vision.blob_detection import detect_blobs\n",
    "\n",
    "from coppeliasim_zmqremoteapi_client import RemoteAPIClient\n",
    "\n",
    "# Init client \n",
    "client = RemoteAPIClient()    # Client object \n",
    "sim = client.getObject('sim') # Simulation object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciating Camera Objects\n",
    "\n",
    "To generate the data regarding the mathematical model of the vision sensor in *CoppeliaSim*, it will be instanciated a Camera Obejcts that matches the Vision Sensors' intrinsic and extrinsic parameters. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cameras = 8\n",
    "camera = [] # Camera objects list\n",
    "\n",
    "for ID in range(n_cameras):\n",
    "    # Get the vision sensor handle\n",
    "    vision_sensor_handle = sim.getObject(f'/VisionSensor[{ID}]')\n",
    "\n",
    "    # Get Coppelia's 3x4 object matrix\n",
    "    object_matrix = np.array(sim.getObjectMatrix(vision_sensor_handle)).reshape((3,4))\n",
    "\n",
    "    # X and Y axis of Coppelia's Vision Sensor are inverted\n",
    "    object_matrix[:,0] *= -1 # Invert x vector column\n",
    "    object_matrix[:,1] *= -1 # Invert y vector column\n",
    "\n",
    "    camera.append(Camera(# Simulation handling\n",
    "                          vision_sensor_handle=vision_sensor_handle,\n",
    "   \n",
    "                          # Intrinsic Parameters\n",
    "                          resolution=(720,720), \n",
    "                          fov_degrees=60.0,     \n",
    "          \n",
    "                          # Extrinsic Parameters\n",
    "                          object_matrix=object_matrix,\n",
    "          \n",
    "                          # Rational Lens Distortion Model\n",
    "                          # distortion_model='rational',\n",
    "                          # distortion_coefficients=np.array([0.014, -0.003, -0.0002, -0.000003, 0.0009, 0.05, -0.007, 0.0017]),\n",
    "\n",
    "                          # Fisheye Lens Distortion Model\n",
    "                          distortion_model='fisheye',\n",
    "                          distortion_coefficients=np.array([0.395, 0.633, -2.417, 2.110]),\n",
    "          \n",
    "                          # Image Noise Model\n",
    "                          snr_dB=13\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Fundamental Matrices\n",
    "\n",
    "For each pair $\\{i, j\\}$, a fundamental matrix will be calculated between camera's $i$ and $j$. In this case, this information will be storted in a matrix $F$ such that $F_{ij}$ will be the fundamental matrix between camera's $i$ and $j$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_matrix = np.array(np.zeros((n_cameras, n_cameras, 3, 3)))\n",
    "\n",
    "for reference in range(n_cameras):\n",
    "    for auxiliary in range(n_cameras):\n",
    "        if reference == auxiliary:\n",
    "            continue\n",
    "\n",
    "        E = build_essential_matrix(camera[reference].extrinsic_matrix, camera[auxiliary].extrinsic_matrix)\n",
    "\n",
    "        F = build_fundamental_matrix(camera[reference].intrinsic_matrix, camera[auxiliary].intrinsic_matrix, E)\n",
    "\n",
    "        fundamental_matrix[reference][auxiliary] = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Simulation\n",
    "\n",
    "For a running simulation, in each Presentation Timestamp (PTS), the following process will happen:\n",
    "\n",
    "1. Get images of all cameras;\n",
    "2. Detect blobs;\n",
    "3. Undistort detected blob coordinates;\n",
    "4. Order blobs based on epipolar lines;\n",
    "5. Triangulate markers.\n",
    "\n",
    "For each PTS the data will be saved for later analysis.\n",
    "\n",
    "To evaluate how well the system is able to process at run-time, the Real Time Factor of the system will be calculated:\n",
    "\n",
    "$${RTF} = \\frac{t_S}{t_R}$$\n",
    "\n",
    "Where $t_S$ is the total simulation time, and $t_R$ is the total ammount of real time that the system took to ran.\n",
    "\n",
    "The `synchronous` flag will define if the *CoppeliaSim* simulation will wait for the trigger to go to its next step. If so, the system is synchronous, if not, the simulation will run by itself and will not wait for the calculations to be done in the code, making the system asynchronous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Time Factor: 0.7903075298224315\n"
     ]
    }
   ],
   "source": [
    "# Synchronous system flag\n",
    "synchronous = True\n",
    "\n",
    "# Make system synchronous\n",
    "sim.setStepping(synchronous)\n",
    "\n",
    "simulation_time = 10 # In seconds\n",
    "simulation_step = sim.getSimulationTimeStep()\n",
    "\n",
    "# Triangulation pair\n",
    "pair = (0, 1)\n",
    "reference, auxiliary = pair\n",
    "\n",
    "images = {} # Images of all cameras in each presentation timestamps\n",
    "true_marker_positions = {} # Ground truth marker positions in each presentation timestamps\n",
    "estimated_marker_positions = {} # Triangulated marker positions in each presentation timestamps\n",
    "\n",
    "# Simulation begins here\n",
    "sim.startSimulation()\n",
    "\n",
    "# Create triangulation tag\n",
    "tag_handle = sim.createDummy(0.015)\n",
    "\n",
    "# Change triangulation tag color properties\n",
    "sim.setObjectColor(tag_handle, 0, sim.colorcomponent_ambient_diffuse, list([0, 0, 0]))\n",
    "sim.setObjectColor(tag_handle, 0, sim.colorcomponent_specular,        list([0, 0, 0])) \n",
    "sim.setObjectColor(tag_handle, 0, sim.colorcomponent_emission,        list([0, 1, 0])) \n",
    "\n",
    "timer_start = time.time() # Start timer\n",
    "\n",
    "while (t := sim.getSimulationTime()) < simulation_time:\n",
    "    # Get and save images\n",
    "    sync_images = [camera[ID].get_coppelia_image(sim.getVisionSensorImg) for ID in pair]\n",
    "    images[t] = sync_images \n",
    "\n",
    "    # Detect blobs on the distorted image\n",
    "    distorted_blobs = [detect_blobs(sync_images[ID]) for ID in pair]\n",
    "\n",
    "    # If no blobs were detected then images are invalid\n",
    "    if any(blob is None for blob in distorted_blobs):\n",
    "        if synchronous:\n",
    "            sim.step()\n",
    "        continue # Jump to next iteration\n",
    "    \n",
    "    # If blob count is different between each image\n",
    "    if distorted_blobs[reference].shape != distorted_blobs[auxiliary].shape:\n",
    "        if synchronous:\n",
    "            sim.step()\n",
    "        continue # Jump to next iteration\n",
    "    \n",
    "    # Undistort blobs\n",
    "    undistorted_blobs = [camera[ID].undistort_points(distorted_blobs[ID].T.reshape(1, -1, 2).astype(np.float32), \n",
    "                                                     camera[ID].intrinsic_matrix, \n",
    "                                                     camera[ID].distortion_coefficients,\n",
    "                                                     np.array([]),\n",
    "                                                     camera[ID].intrinsic_matrix).reshape(-1,2).T for ID in pair]\n",
    "\n",
    "    # Order blobs\n",
    "    epilines_auxiliary = cv2.computeCorrespondEpilines(points=undistorted_blobs[reference].T, \n",
    "                                                       whichImage=1, \n",
    "                                                       F=fundamental_matrix[reference][auxiliary]).reshape(-1,3)\n",
    "    \n",
    "    undistorted_blobs[auxiliary] = order_centroids(undistorted_blobs[auxiliary], epilines_auxiliary)\n",
    "\n",
    "    # Triangulate markers\n",
    "    triangulated_positions = cv2.triangulatePoints(camera[reference].projection_matrix.astype(float), \n",
    "                                                   camera[auxiliary].projection_matrix.astype(float), \n",
    "                                                   undistorted_blobs[reference].astype(float), \n",
    "                                                   undistorted_blobs[auxiliary].astype(float))\n",
    "    \n",
    "    # Normalize homogeneous coordinates and save\n",
    "    triangulated_positions = (triangulated_positions / triangulated_positions[-1])[:-1, :]\n",
    "    estimated_marker_positions[t] = triangulated_positions\n",
    "\n",
    "    # Position triangulation tag\n",
    "    sim.setObjectPosition(tag_handle, triangulated_positions.reshape(-1, 3)[0].tolist())\n",
    "\n",
    "    # Get ground truth marker position for comparison\n",
    "    true_marker_position = np.array(sim.getObjectPosition(sim.getObject('/Marker'))).reshape(3,-1)\n",
    "    true_marker_positions[t] = true_marker_position\n",
    "\n",
    "    # Call next simulation step\n",
    "    if synchronous:\n",
    "        sim.step()\n",
    "\n",
    "timer_end = time.time() # Stop timer\n",
    "\n",
    "# Simulation ends here\n",
    "sim.stopSimulation()\n",
    "\n",
    "print(f'Real Time Factor: {simulation_time / (timer_end - timer_start)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Precision and Accuracy \n",
    "\n",
    "Since the ground truth data of the marker's position is available, it's possible to evaluate the Precision and Accuracy of the system for the capture of a single stationary marker on the scene.\n",
    "\n",
    "Calculating the average difference between the position of the marker it's ground truth will determine the **Precision**. Calculating the standard deviation of the marker's position will determine how accurate the system is by it's **Jitter**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reconstruction Error: 23.02 mm\n"
     ]
    }
   ],
   "source": [
    "static_capture = False # Flag if the marker is moving or not in the scene \n",
    "\n",
    "# Gathering Data\n",
    "all_marker_positions = None\n",
    "triangulation_errors = None\n",
    "for frame, pts in enumerate(estimated_marker_positions.keys()):\n",
    "    if all_marker_positions is None:\n",
    "        all_marker_positions = estimated_marker_positions[pts]\n",
    "    else:\n",
    "        all_marker_positions = np.hstack((all_marker_positions, estimated_marker_positions[pts]))\n",
    "\n",
    "    if triangulation_errors is None:\n",
    "        triangulation_errors = np.linalg.norm(estimated_marker_positions[pts] - true_marker_positions[pts])\n",
    "    else:\n",
    "        triangulation_errors = np.hstack((triangulation_errors, np.linalg.norm(estimated_marker_positions[pts] - true_marker_positions[pts])))\n",
    "\n",
    "# For static single marker capture\n",
    "if static_capture == True:\n",
    "    true_position = np.ravel(list(true_marker_positions.values())[0]) # Ground truth\n",
    "    average_position = np.mean(all_marker_positions, axis=1) # For Precision\n",
    "    std_dev_position = np.std(all_marker_positions, axis=1)  # For Accuracy \n",
    "\n",
    "    print('Ground Truth Position:', true_position)\n",
    "    print('Average Position:', average_position) \n",
    "    print(f'Average Reconstruction Error: {np.mean(triangulation_errors) * 1e3 :.2f} mm') # Precision\n",
    "    print(f'Standard Deviation: {np.linalg.norm(std_dev_position) * 1e3 :.2f} mm')        # Accuracy\n",
    "\n",
    "# For moving single marker capture\n",
    "else:\n",
    "    print(f'Average Reconstruction Error: {np.mean(triangulation_errors) * 1e3 :.2f} mm') # Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Back the Camera's Feed\n",
    "\n",
    "The following cell will replicate a real time camera feed of the simulation. Change the `ID` parameter to switch between camera views.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playback a camera image feed in real time\n",
    "ID = 0 # Camera ID to be watched\n",
    "timestep = int(1e3 * simulation_time/len(images.keys())) # In milliseconds\n",
    "for frame, pts in enumerate(images.keys()):\n",
    "    cv2.imshow(f'Camera {ID}', images[pts][ID])\n",
    "    cv2.waitKey(timestep)\n",
    "\n",
    "# Closing all open windows \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
